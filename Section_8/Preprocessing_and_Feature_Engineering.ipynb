{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for machine learning algorithms to work properly they require their input data to be formatted in certain ways. The preprocessing phase handles these formatting requirements through a number of different techniques such as binarization, mean removal, scaling, normalization, and label encoding. Similarly, feature engineering is about taking any present information and turning it into a numerically illustrative feature matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Numerical values are transformed into Boolean values represented by zeros and ones.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/preprocessing.html**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4192486   0.80649579]\n",
      " [ 0.85135686  0.86483814]\n",
      " [ 0.51005527  0.15094046]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create random test set.\n",
    "inputData = np.random.rand(3, 2)\n",
    "print(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized Data:\n",
      " [[0 1]\n",
      " [1 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Binarize data with values over .5 being 1 and those below 0.\n",
    "binData = np.where(inputData > .5, 1, 0)\n",
    "print(\"Binarized Data:\\n\", binData)\n",
    "\n",
    "# Alternative:\n",
    "# inputData = inputData.reshape(-1, 1)\n",
    "# data_binarized = preprocessing.Binarizer(threshold=.5).transform(inputData) \n",
    "# print(\"\\nBinarized Data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mean Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Removing the mean involves setting it to zero and having a standard deviation of ones. This helps reduce bias in the input data.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      " [[ 0.4192486   0.80649579]\n",
      " [ 0.85135686  0.86483814]\n",
      " [ 0.51005527  0.15094046]]\n",
      "\n",
      "Mean: [ 0.59355358  0.60742479]\n",
      "Standard Deviation: [ 0.18602573  0.32366075]\n"
     ]
    }
   ],
   "source": [
    "# Mean and standard deviation before mean removal.\n",
    "print(\"Input Data:\\n\", inputData)\n",
    "print(\"\\nMean:\", inputData.mean(axis=0))\n",
    "print(\"Standard Deviation:\", inputData.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      " [[-0.93699388  0.61506065]\n",
      " [ 1.38584739  0.79531839]\n",
      " [-0.44885351 -1.41037904]]\n",
      "\n",
      "Mean: [ -1.85037171e-17   1.48029737e-16]\n",
      "Standard Deviation: [ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Mean and standard deviation after mean removal.\n",
    "scaledData = preprocessing.scale(inputData)\n",
    "print(\"Input Data:\\n\", scaledData)\n",
    "print(\"\\nMean:\", scaledData.mean(axis=0))\n",
    "print(\"Standard Deviation:\", scaledData.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**As the name implies this technique helps to scale down the variance of the input data so as to avoid any artificially large or small inputs.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      " [[ 0.4192486   0.80649579]\n",
      " [ 0.85135686  0.86483814]\n",
      " [ 0.51005527  0.15094046]]\n",
      "\n",
      "Min max scaled data:\n",
      " [[ 0.          0.91827631]\n",
      " [ 1.          1.        ]\n",
      " [ 0.21014797  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Min max scaling with a feature range from 0 and 1.\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(inputData)\n",
    "print(\"Input Data:\\n\", inputData)\n",
    "print(\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**This process regulates the values in the input data to a common scale. L1 normalization, or Least Absolute Deviations, makes it so that the sum of absolute values in each row is equal to one. L2 normalization, or least squares, has a similar approach by evaluating that the sum of the squared row values is equal to one. L1 normalization tends to be more resistant to outliers.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Normalized Data:\n",
      " [[ 0.34203591  0.65796409]\n",
      " [ 0.49607233  0.50392767]\n",
      " [ 0.77164685  0.22835315]]\n",
      "\n",
      "L2 Normalized Data:\n",
      " [[ 0.46124084  0.88727498]\n",
      " [ 0.70153058  0.71263935]\n",
      " [ 0.95889383  0.28376508]]\n"
     ]
    }
   ],
   "source": [
    "L1NormalizedData = preprocessing.normalize(inputData, norm='l1')\n",
    "L2NormalizedData = preprocessing.normalize(inputData, norm='l2')\n",
    "print(\"L1 Normalized Data:\\n\", L1NormalizedData)\n",
    "print(\"\\nL2 Normalized Data:\\n\", L2NormalizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Labels are often times an attribute of data sets, and sometimes these labels are in a non-numeric format. To prepare the data to be used in machine learning functions it is necessary to covert word labels into numbers though the use of a label encoder.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping:\n",
      "AAPL --> 0\n",
      "AMD --> 1\n",
      "MSFT --> 2\n",
      "NVDA --> 3\n"
     ]
    }
   ],
   "source": [
    "# Sample input labels.\n",
    "inputLabels = ['NVDA', 'AAPL', 'AMD', 'MSFT'] \n",
    "\n",
    "# Create a label encoder and fit the labels. \n",
    "encoder = preprocessing.LabelEncoder() \n",
    "encoder.fit(inputLabels) \n",
    "\n",
    "# Print the mapping.\n",
    "print(\"\\nLabel Mapping:\") \n",
    "for i, item in enumerate(encoder.classes_): \n",
    "    print(item, '-->', i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Labels = ['AMD', 'NVDA', 'AAPL', 'MSFT']\n",
      "Encoded Values = [1, 3, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Encode a set of labels using the encoder.\n",
    "testLabels = ['AMD', 'NVDA', 'AAPL', 'MSFT'] \n",
    "encodedValues = encoder.transform(testLabels) \n",
    "print(\"\\nTest Labels =\", testLabels) \n",
    "print(\"Encoded Values =\", list(encodedValues)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imputation of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**When there is missing data in an array a common approach is to replace the missing value with something appropriate. This approach is referred to as imputation of missing values, and there are a variety of techniques that can be used to accomplish this. Basic imputation involves replacing the missing value with a mean, median, or most frequent value strategy.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.5   1.    5.  ]\n",
      " [ 9.    7.    3.  ]\n",
      " [ 1.    5.    3.75]\n",
      " [ 4.    2.    6.  ]\n",
      " [ 8.    3.75  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import nan\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "trainingData = np.array([1, 8, 4, 3, 7])\n",
    "incompleteData = np.array([[ nan, 1, 5 ],\n",
    "                           [ 9, 7, 3 ],\n",
    "                           [ 1, 5, nan ],\n",
    "                           [ 4, 2, 6 ],\n",
    "                           [ 8, nan, 1 ]])\n",
    "# Select strategy.\n",
    "imputer = Imputer(strategy='mean')\n",
    "# Apply transformation.\n",
    "fixedData = imputer.fit_transform(incompleteData)\n",
    "print(fixedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The missing data has been replaced with values representing a mean of the relevant column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transforming Inputs Through Derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Sometimes the input data just isn’t enough to be described well by certain modelling methods. A way to get around this is by deriving and adding additional features to the data in order to improve model flexibility. Model improvement can therefore be done by transforming the inputs instead of changing the model itself.**\n",
    "\n",
    "**More Information: https://onlinecourses.science.psu.edu/stat501/node/251**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These commands are for Azure users to run. Uncomment the following to have them enabled when running this cell.\n",
    "# !pip install matplotlib\n",
    "# !conda install nomkl -y\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x254fe2c2198>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASAZwg0AyRBuAEjm/wHWKv4LHHD50QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x254fe228a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and plot example coordinates.\n",
    "x = np.array([1, 2, 4, 5, 7, 9, 10, 13])\n",
    "y = np.array([3, 5, 3, 2, 4, 7, 6, 4])\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x254fe296b70>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkZD5/49ab6H4WC2cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x254fe2969b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2D array.\n",
    "X = x.reshape(-1, 1)\n",
    "# Fit a regression line to the data points.\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   1.00000000e+00   1.00000000e+00]\n",
      " [  2.00000000e+00   4.00000000e+00   8.00000000e+00]\n",
      " [  4.00000000e+00   1.60000000e+01   6.40000000e+01]\n",
      " [  5.00000000e+00   2.50000000e+01   1.25000000e+02]\n",
      " [  7.00000000e+00   4.90000000e+01   3.43000000e+02]\n",
      " [  9.00000000e+00   8.10000000e+01   7.29000000e+02]\n",
      " [  1.00000000e+01   1.00000000e+02   1.00000000e+03]\n",
      " [  1.30000000e+01   1.69000000e+02   2.19700000e+03]]\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial features.\n",
    "polynomial = PolynomialFeatures(degree=3, include_bias=False)\n",
    "additionalFeaturesData = polynomial.fit_transform(X)\n",
    "\n",
    "# The feature matrix has the first column representing the original x values, \n",
    "# the second column representing x squared, and the third column representing x cubed.\n",
    "print(additionalFeaturesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x254fe2fb208>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x254fc10f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a more sophisticated model with the additional data using linear regression.\n",
    "model = LinearRegression().fit(additionalFeaturesData, y)\n",
    "yfit = model.predict(additionalFeaturesData)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## One-Hot Encoding of Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A more useful technique to altering non-numeric, or categorical, data into a numbered format is by using one-hot encoding. This approach creates extra columns in the data that represent the presence or absence of categorical features through the labels 0 or 1. Encoding like this can increase a data set dramatically if it contained a lot of varied categorical features.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/feature_extraction.html**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'price' : 100, 'quantity' : 1, 'model' : 'ZX42L'},\n",
    "        {'price' : 50, 'quantity' : 3, 'model' : 'Y7S3'},\n",
    "        {'price' : 75, 'quantity' : 2, 'model' : 'SCR77'},\n",
    "        {'price' : 23, 'quantity' : 4, 'model' : 'LCM28R'},\n",
    "        {'price' : 70, 'quantity' : 2, 'model' : 'SCR77'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   1, 100,   1],\n",
       "       [  0,   0,   1,   0,  50,   3],\n",
       "       [  0,   1,   0,   0,  75,   2],\n",
       "       [  1,   0,   0,   0,  23,   4],\n",
       "       [  0,   1,   0,   0,  70,   2]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Transforms dictionary to vectors.\n",
    "vectorizer = DictVectorizer(sparse=False, dtype=int)\n",
    "\n",
    "# The 'model' column is expanded into four separate columns based on the existing four label types, \n",
    "# and a 1 is present on the row that is associated with one of those labels. \n",
    "vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model=LCM28R',\n",
       " 'model=SCR77',\n",
       " 'model=Y7S3',\n",
       " 'model=ZX42L',\n",
       " 'price',\n",
       " 'quantity']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature (column) names.\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**It is also possible to encode text data into numerical values by word counts with Scikit-Learn’s CountVectorizer. The occurrences of each word in each piece of text is counted and organized in a table.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['What is your itinerary',\n",
    "        'To meet my maker',\n",
    "        'And what do you want to say to your maker',\n",
    "        'The coward dies a thousand deaths',\n",
    "        'The valiant taste of death but once']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>but</th>\n",
       "      <th>coward</th>\n",
       "      <th>death</th>\n",
       "      <th>deaths</th>\n",
       "      <th>dies</th>\n",
       "      <th>do</th>\n",
       "      <th>is</th>\n",
       "      <th>itinerary</th>\n",
       "      <th>maker</th>\n",
       "      <th>...</th>\n",
       "      <th>say</th>\n",
       "      <th>taste</th>\n",
       "      <th>the</th>\n",
       "      <th>thousand</th>\n",
       "      <th>to</th>\n",
       "      <th>valiant</th>\n",
       "      <th>want</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  but  coward  death  deaths  dies  do  is  itinerary  maker  ...   say  \\\n",
       "0    0    0       0      0       0     0   0   1          1      0  ...     0   \n",
       "1    0    0       0      0       0     0   0   0          0      1  ...     0   \n",
       "2    1    0       0      0       0     0   1   0          0      1  ...     1   \n",
       "3    0    0       1      0       1     1   0   0          0      0  ...     0   \n",
       "4    0    1       0      1       0     0   0   0          0      0  ...     0   \n",
       "\n",
       "   taste  the  thousand  to  valiant  want  what  you  your  \n",
       "0      0    0         0   0        0     0     1    0     1  \n",
       "1      0    0         0   1        0     0     0    0     0  \n",
       "2      0    0         0   2        0     1     1    1     1  \n",
       "3      0    1         1   0        0     0     0    0     0  \n",
       "4      1    1         0   0        1     0     0    0     0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Transforms text to a matrix of token counts.\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(data)\n",
    "\n",
    "# Convert sparse matrix to a DataFrame with labeled columns.\n",
    "pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**To avoid putting too much weight on words that appear very frequently the term frequency–inverse document frequency (TF–IDF) technique can be used. It weights the word counts by a measure of how often they appear in the documents.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>but</th>\n",
       "      <th>coward</th>\n",
       "      <th>death</th>\n",
       "      <th>deaths</th>\n",
       "      <th>dies</th>\n",
       "      <th>do</th>\n",
       "      <th>is</th>\n",
       "      <th>itinerary</th>\n",
       "      <th>maker</th>\n",
       "      <th>...</th>\n",
       "      <th>say</th>\n",
       "      <th>taste</th>\n",
       "      <th>the</th>\n",
       "      <th>thousand</th>\n",
       "      <th>to</th>\n",
       "      <th>valiant</th>\n",
       "      <th>want</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550329</td>\n",
       "      <td>0.550329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.260985</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.260985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.312840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        and       but    coward     death    deaths      dies        do  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.323484  0.000000  0.000000  0.000000  0.000000  0.000000  0.323484   \n",
       "3  0.000000  0.000000  0.463693  0.000000  0.463693  0.463693  0.000000   \n",
       "4  0.000000  0.387757  0.000000  0.387757  0.000000  0.000000  0.000000   \n",
       "\n",
       "         is  itinerary     maker    ...          say     taste       the  \\\n",
       "0  0.550329   0.550329  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.444002    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000   0.000000  0.260985    ...     0.323484  0.000000  0.000000   \n",
       "3  0.000000   0.000000  0.000000    ...     0.000000  0.000000  0.374105   \n",
       "4  0.000000   0.000000  0.000000    ...     0.000000  0.387757  0.312840   \n",
       "\n",
       "   thousand        to   valiant      want      what       you      your  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.444002  0.000000  0.444002  \n",
       "1  0.000000  0.444002  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.521969  0.000000  0.323484  0.260985  0.323484  0.260985  \n",
       "3  0.463693  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.387757  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(data)\n",
    "pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**For the purposes of stringing together multiple steps Scikit-Learn provides a pipeline object that can be used to streamline a processing pipeline.**\n",
    "\n",
    "**Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: [1 8 4 3 7]\n",
      "Predicted: [ 1.  8.  4.  3.  7.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(Imputer(strategy='mean'), # Impute missing values using the mean.\n",
    "                      PolynomialFeatures(degree=2), # Transform features to quadratic.                  \n",
    "                      LinearRegression()) # Fit a linear regression model.\n",
    "\n",
    "# Predict results by applying the model to the training data.\n",
    "model.fit(incompleteData, trainingData)\n",
    "print('Actual:', trainingData)\n",
    "print('Predicted:', model.predict(incompleteData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "> Joshi, P. (2017). Artificial Intelligence with Python (1st ed.). (S. E. Vikrant Phadkay, Ed.) Birmingham, United Kingdom: Packt Publishing.\n",
    "\n",
    "> Pennsylvania State University. (2018). What is Simple Linear Regression? Retrieved from onlinecourses.science.psu.edu: https://onlinecourses.science.psu.edu/stat501/node/251\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). Feature extraction, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). Preprocessing data, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.feature_extraction.DictVectorizer, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.feature_extraction.text.CountVectorizer, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.feature_extraction.text.TfidfVectorizer, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.linear_model.LinearRegression, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.pipeline.make_pipeline, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.Binarizer, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.Imputer, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.LabelEncoder, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.MinMaxScaler, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.normalize, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.PolynomialFeatures, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "> Scikit-learn Developers. (2017, October). sklearn.preprocessing.scale, 0.19.1. Retrieved from scikit-learn.org: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\n",
    "\n",
    "> VanderPlas, J. (2016). Python Data Science Handbook (1st ed.). (D. Schanafelt, Ed.) Sebastopol, California, United States of America: O’Reilly Media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
